{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\ngc.enable()\nimport glob\nimport random\nimport numpy as np \nimport pandas as pd  \nimport json\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.optim import lr_scheduler\nfrom tensorflow.keras.utils import pad_sequences\nfrom sklearn.metrics import accuracy_score\nimport lzma\nimport pickle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_colwidth', 150)\n\ndef set_seeed(seed_value=23, use_cuda=True):\n    np.random.seed(seed_value) \n    torch.manual_seed(seed_value) \n    random.seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    if use_cuda: \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) \n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_seeed()\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    CREATE_DATA = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reader(f):\n    try:\n        df = pd.read_csv(f)\n        df['ID']=f.split('/')[-1].split('.')[0]\n        return df\n    except: pass\n\ntrain_files_p = '/kaggle/input/network-traffic-scenario-prediction/Train_data/Train_data'\ntest_files_p = '/kaggle/input/network-traffic-scenario-prediction/Test_data/Test_data'\nss_p = '/kaggle/input/network-traffic-scenario-prediction/SampleSubmission.csv'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain_files = glob.glob(train_files_p+'/**')\ndfs = []\nfor f in tqdm(train_files):\n    dfs.append(reader(f))\ntrain = pd.concat(dfs).fillna(0)\ndel dfs\ngc.collect()\n\n\ntest_files = glob.glob(test_files_p+'/**')\ndfs = []\nfor f in tqdm(test_files):\n    dfs.append(reader(f))\ntest = pd.concat(dfs).fillna(0)\ndel dfs\ngc.collect()\n\nss = pd.read_csv(ss_p)\n\ndisplay(train, test, ss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['ID'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['ID'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['label'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BidirectionalLSTMClassifier(nn.Module):\n    def __init__(self, input_size, num_classes, hidden_size, num_layers):\n        super().__init__()\n\n        self.Lstm_layer_1 = nn.GRU(input_size=input_size,\n                                    hidden_size=hidden_size,\n                                    num_layers=num_layers,\n                                    bidirectional=True,\n                                    batch_first=True)\n\n        self.Output = nn.Linear(in_features=self.Lstm_layer_1.hidden_size*2, out_features=num_classes)\n\n    def forward(self, inputs):\n        lstm_1_seq, _ = self.Lstm_layer_1(inputs)\n        output = self.Output(lstm_1_seq)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.sort_values('time').reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_length = 5000\ntrain['sequence_id'] = train['time'] // sequence_length\ntrain['sequence_id'] = train['sequence_id'].astype(str)\ntrain['sequence_id_ID'] = train['sequence_id'] + '_' + train['ID']\ntrain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['sequence_id_ID'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['sequence_id_ID']=='0_Train53']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ids = list(train['ID'].unique())\ntrain_ids, val_ids = train_test_split(train_ids, random_state=1, test_size=0.15)\nlen(train_ids), len(val_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = np.array(list(set(train['label'].unique())))\nlabels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain_df = train.copy()\n\nxtrain_df = train_df[train_df['ID'].isin(train_ids)].reset_index(drop=True)\nxval_df = train_df[train_df['ID'].isin(val_ids)].reset_index(drop=True)\n\nfeatures = ['time','portPktIn','portPktOut','qSize']\nscaler = RobustScaler() \nxtrain_df[features] = scaler.fit_transform(xtrain_df[features])\nxval_df[features] = scaler.transform(xval_df[features])\n\nxtrain_df.fillna(0,inplace=True)\nxval_df.fillna(0,inplace=True)\n\nxtrain_df.replace([np.inf,-np.inf],0,inplace=True)\nxval_df.replace([np.inf,-np.inf],0,inplace=True)\n    \ndel train,train_df\ngc.collect()\n    \nif CFG.CREATE_DATA:\n    xtrain_idids = xtrain_df['sequence_id_ID'].unique()\n    xval_idids = xval_df['sequence_id_ID'].unique()\n\n    xtrain_data = []\n    xtrain_targets = []\n    for d in tqdm(xtrain_idids):\n        data = xtrain_df[xtrain_df['sequence_id_ID']==d].sort_values('time')\n        inputs = data[features].values\n        inputs = pad_sequences([inputs], maxlen=sequence_length, dtype='float', padding='post', value=-1)[0,:,:]\n        xtrain_data.append(inputs)\n\n        targets = data['label'].values\n        targets = pad_sequences([targets], maxlen=sequence_length, dtype='int', padding='post', value=12)[0]\n        xtrain_targets.append(targets)\n\n    xval_data = []\n    xval_targets = []\n    for d in tqdm(xval_idids):\n        data = xval_df[xval_df['sequence_id_ID']==d].sort_values('time')\n        inputs = data[features].values\n        inputs = pad_sequences([inputs], maxlen=sequence_length, dtype='float', padding='post', value=-1)[0,:,:]\n        xval_data.append(inputs)\n\n        targets = data['label'].values\n        targets = pad_sequences([targets], maxlen=sequence_length, dtype='int', padding='post', value=12)[0]\n        xval_targets.append(targets)\n\n    xtrain_data = np.array(xtrain_data)\n    xval_data = np.array(xval_data)\n\n    xtrain_targets = np.array(xtrain_targets)\n    xval_targets = np.array(xval_targets)\n    \n    with open('xtrain_data.npy','wb') as f:\n        np.save(f, xtrain_data)\n\n    with open('xval_data.npy','wb') as f:\n        np.save(f, xval_data)\n\n    with open('xtrain_targets.npy','wb') as f:\n        np.save(f, xtrain_targets)\n\n    with open('xval_targets.npy','wb') as f:\n        np.save(f, xval_targets)\n\nelse:\n    \n    saved_path = '/kaggle/input/network-traffic-classification-lstm-data/'\n    with open(saved_path+'xtrain_data.npy','rb') as f:\n        xtrain_data = np.load(f)\n\n    with open(saved_path+'xval_data.npy','rb') as f:\n        xval_data = np.load(f)\n\n    with open(saved_path+'xtrain_targets.npy','rb') as f:\n        xtrain_targets = np.load(f)\n\n    with open(saved_path+'xval_targets.npy','rb') as f:\n        xval_targets = np.load(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MySequenceDataset(Dataset):\n    def __init__(self, df, features=['time','portPktIn','portPktOut','qSize'], sequence_length=sequence_length):\n        \n        self.df = df\n        self.seq_id_IDs = self.df['sequence_id_ID'].unique()\n        self.features = features\n        self.sequence_length = sequence_length\n        \n    def __len__(self):\n        return len(self.seq_id_IDs)\n\n    def __getitem__(self, idx):\n        seq_id_ID = self.seq_id_IDs[idx]\n        \n        data = self.df[self.df['sequence_id_ID']==seq_id_ID].sort_values('time')\n        inputs = data[self.features].values\n        inputs = pad_sequences([inputs], maxlen=self.sequence_length, dtype='float', padding='post', value=-1)[0,:,:]\n        targets = data['label'].values\n        targets = pad_sequences([targets], maxlen=self.sequence_length, dtype='int', padding='post', value=12)[0]\n        \n        return torch.tensor(inputs, dtype=torch.float32), torch.tensor(targets, dtype=torch.long)\n    \nclass MySimpleSequenceDataset(Dataset):\n    def __init__(self, data, targets):\n        \n        self.data = data\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        inputs = self.data[idx]\n        targets = self.targets[idx]\n        \n        return torch.tensor(inputs, dtype=torch.float32), torch.tensor(targets, dtype=torch.long)\n    \ndef save_checkpoint(checkpoint, filename):\n    torch.save(checkpoint, filename)\n    print(f\"\\n--> Saved checkpoint: {filename.split('.')[0]}\")\n\ndef load_checkpoint(filename, model):\n    model.load_state_dict(torch.load(filename)['state_dict'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\n\ntrain_ds = MySimpleSequenceDataset(xtrain_data, xtrain_targets)\nval_ds = MySimpleSequenceDataset(xval_data, xval_targets)\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BidirectionalLSTMClassifier(nn.Module):\n    def __init__(self, input_size, num_classes, hidden_size, num_layers):\n        super().__init__()\n\n        self.Lstm_layer_1 = nn.GRU(input_size=input_size,\n                                    hidden_size=hidden_size,\n                                    num_layers=num_layers,\n                                    bidirectional=True,\n                                    batch_first=True)\n\n        self.Output = nn.Linear(in_features=self.Lstm_layer_1.hidden_size*2, out_features=num_classes)\n\n    def forward(self, inputs):\n        lstm_1_seq, _ = self.Lstm_layer_1(inputs)\n        output = self.Output(lstm_1_seq)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nnum_classes = len(labels)+1\ninput_size = len(features)  \nhidden_size = 16\nnum_layers = 1   \n\nmodel = BidirectionalLSTMClassifier(input_size, num_classes, hidden_size, num_layers).to(device)\n\ncriterion = nn.CrossEntropyLoss().to(device)\n\nfor x,y in train_loader:\n    break\n    \noutput = model(x.to(device))\nloss = criterion(output.view(-1, num_classes), y.to(device).view(-1))\nprint(loss)\n_, predicted = torch.max(output, 2)\nprint(output.shape)\nprint(predicted)\nprint(predicted.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nepochs = 200\nbatch_size = 4\nhidden_size = 64\nnum_layers = 3\n\ntrain_ds = MySimpleSequenceDataset(xtrain_data, xtrain_targets)\nval_ds = MySimpleSequenceDataset(xval_data, xval_targets)\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n\ntorch.cuda.empty_cache()\ngc.collect()\n\ninput_size = len(features)  \nnum_classes = len(labels)+1\nmodel = BidirectionalLSTMClassifier(input_size, num_classes, hidden_size, num_layers).to(device)\n\nmodel_filename = 'network_traffic.pth'\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=4, verbose=True, min_lr=1e-6)\n\nbest_val_accuracy = 0\nbest_val_targets = []\nbest_val_preds = []\nepochs_without_improvement = 0\npatience = 10\n\nfor epoch in range(epochs):\n    dataset_size = 0\n    running_loss = 0.0\n    model.train()\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc='Train ')\n    for step,(inputs, targets) in pbar:\n        inputs,targets = inputs.to(device), targets.to(device)\n\n        batch_size = inputs.size(0)\n        \n        outputs = model(inputs)\n        loss = criterion(outputs.view(-1, num_classes), targets.view(-1))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss / dataset_size\n        \n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        current_lr = optimizer.param_groups[0]['lr']\n        pbar.set_postfix(train_loss=f'{epoch_loss:0.4f}',\n                         lr=f'{current_lr:0.5f}',\n                         gpu_mem=f'{mem:0.2f} GB')\n        \n    torch.cuda.empty_cache()\n    gc.collect()\n\n    model.eval()\n    with torch.no_grad():\n        all_predictions = []\n        all_targets = []\n        \n        dataset_size = 0\n        running_loss = 0.0\n        pbar = tqdm(enumerate(val_loader), total=len(val_loader), desc='Valid ')\n        for step, (inputs, targets) in pbar:\n            inputs,targets = inputs.to(device), targets.to(device)\n            batch_size = inputs.size(0)\n            \n            outputs = model(inputs)\n            \n            loss = criterion(outputs.view(-1, num_classes), targets.view(-1))\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n\n            epoch_loss = running_loss / dataset_size\n\n            mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n            current_lr = optimizer.param_groups[0]['lr']\n            pbar.set_postfix(val_loss=f'{epoch_loss:0.4f}',\n                             lr=f'{current_lr:0.5f}',\n                             gpu_mem=f'{mem:0.2f} GB')\n            \n            _, predicted = torch.max(outputs, 2)\n            all_predictions.extend(predicted.cpu().numpy().ravel())\n            all_targets.extend(targets.cpu().numpy().ravel())\n    \n    all_targets_ = np.array(all_targets)\n    all_predictions_ = np.array(all_predictions)\n\n    all_predictions_ = all_predictions_[np.where(all_targets_!=12)]\n    all_targets_ = all_targets_[np.where(all_targets_!=12)]\n\n    val_accuracy = accuracy_score(all_targets_, all_predictions_)\n    print(f\"Epoch {epoch+1}/{epochs}, Accuracy Score: {val_accuracy:.4f}\")\n    \n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        best_val_targets = all_targets_\n        best_val_preds = all_predictions_\n        epochs_without_improvement = 0\n        checkpoint = {'state_dict': model.state_dict(),'optimizer': optimizer.state_dict()}\n        save_checkpoint(checkpoint=checkpoint, filename=model_filename)\n    else:\n        epochs_without_improvement +=1\n\n    if epochs_without_improvement == patience:\n        break\n            \n    scheduler.step(val_accuracy)\n    \n    torch.cuda.empty_cache()\n    gc.collect()\n    \ndel xtrain_data, xtrain_targets, xval_data, xval_targets\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('BEST VAL ACCURACY: ', np.round(best_val_accuracy,5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MySimpleSequenceDatasetInference(Dataset):\n    def __init__(self, data):\n        \n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        inputs = self.data[idx]\n        \n        return torch.tensor(inputs, dtype=torch.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_files = glob.glob(test_files_p+'/**')\ndfs = []\nfor f in tqdm(test_files):\n    dfs.append(reader(f))\ntest = pd.concat(dfs).fillna(0)\ndel dfs\ngc.collect()\n\ntest['time'] = test['time'].astype(int).astype(str)\ntest['ID2'] = test['ID'].str.replace('T','t') +\"_\"+ test['time'] \ntest['time'] = test['time'].astype(int)\n\ndisplay(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test.copy()\ntest_df['sequence_id'] = test_df['time'] // sequence_length\ntest_df['sequence_id'] = test_df['sequence_id'].astype(str)\ntest_df['sequence_id_ID'] = test_df['sequence_id'] + '_' + test_df['ID']\ntest_df[features] = scaler.transform(test_df[features])\ndisplay(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtest_idids = test_df['sequence_id_ID'].unique()\n\nxtest_data = []\ntmp_data = []\ntest_id_data = []\nfor d in tqdm(xtest_idids):\n    data = test_df[test_df['sequence_id_ID']==d].sort_values('time')\n    inputs = data[features].values\n    inputs = pad_sequences([inputs], maxlen=sequence_length, dtype='float', padding='post', value=-1)[0,:,:]\n    xtest_data.append(inputs)\n    \n    tmp_ = data['time'].values\n    tmp_ = pad_sequences([tmp_], maxlen=sequence_length, dtype='float', padding='post', value=-1)[0]\n    tmp_data.append(tmp_)\n    \n    test_id_data_ = data['ID2'].values\n    test_id_data_ = pad_sequences([test_id_data_], maxlen=sequence_length, dtype=object, padding='post', value='0')[0]\n    test_id_data.append(test_id_data_)\n    \nxtest_data = np.array(xtest_data)\ntmp_data = np.array(tmp_data)\ntest_id_data = np.array(test_id_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nmodel_filename = f'network_traffic.pth'\nbatch_size = 16\n\ntest_ds = MySimpleSequenceDatasetInference(xtest_data)\ntest_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n\nhidden_size = 64\nnum_layers = 3\ninput_size = len(features)  \nnum_classes = len(labels)+1\nmodel = BidirectionalLSTMClassifier(input_size, num_classes, hidden_size, num_layers).to(device)          \nmodel = load_checkpoint(filename=model_filename, model=model)\n\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions = []\nmodel.eval()\nwith torch.no_grad():\n    pbar = tqdm(enumerate(test_loader), total=len(test_loader), desc='Inference ')\n    for step, inputs in pbar:\n        inputs = inputs.to(device)\n        outputs = model(inputs)\n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        pbar.set_postfix(gpu_mem=f'{mem:0.2f} GB')\n            \n        _, predicted = torch.max(outputs, 2)\n        all_predictions.extend(predicted.cpu().numpy().ravel())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_id_data_ = test_id_data.ravel()\nall_predictions_ = np.array(all_predictions)\n\nlen(all_predictions_), len(test_id_data_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame(all_predictions_)\nsub['ID'] = test_id_data_\nsub.columns = ['Target','ID']\nsub = sub[['ID','Target']]\n\nsub = sub[sub['ID']!='0']\nsub.loc[sub['Target']==12,'Target'] = 8\nsub_ = pd.merge(ss[['ID']],sub[['ID', 'Target']], how='left', on='ID')\nsub_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_.to_csv('sub.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_['Target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}